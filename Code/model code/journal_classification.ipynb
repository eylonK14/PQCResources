{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journal Classification Analysis\n",
    "\n",
    "This notebook performs classification analysis on journal data using various machine learning models. It includes:\n",
    "- Data loading and preprocessing\n",
    "- Multiple classification model evaluation\n",
    "- Performance metrics calculation\n",
    "- Learning curve visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import ast\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, LearningCurveDisplay, ShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target: str, csv_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load and preprocess data from a CSV file.\n",
    "    :param target: One of 'all', 'pqc', 'browser', 'os', or 'algo'.\n",
    "    :param csv_path: Path to the CSV file to load.\n",
    "    :return: Tuple (data, labels) where data is a numpy array and labels is another numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV at '{csv_path}': {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    labels = data.pop('label').values\n",
    "    \n",
    "    # Log label counts\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(\"Label distribution:\", dict(zip(unique, counts)))\n",
    "    \n",
    "    rows = data.to_numpy()\n",
    "    filtered_rows, filtered_labels = [], []\n",
    "    \n",
    "    for row, label in zip(rows, labels):\n",
    "        unit_digit = label % 10\n",
    "        \n",
    "        # Filter based on target\n",
    "        if target == 'pqc':\n",
    "            # PQC includes all samples (unit digits 0, 1, 2)\n",
    "            pass\n",
    "        elif target in ['browser', 'os', 'algo']:\n",
    "            # These targets only include samples with unit digits 1 or 2\n",
    "            if unit_digit == 0:\n",
    "                continue\n",
    "        elif target == 'all':\n",
    "            # All includes everything\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Unknown target: {target}\")\n",
    "            return None, None\n",
    "        \n",
    "        filtered_rows.append([ast.literal_eval(cell) for cell in row])\n",
    "        filtered_labels.append(label)\n",
    "    \n",
    "    X = np.array([np.array(r).flatten() for r in filtered_rows])\n",
    "    y = np.array(filtered_labels)\n",
    "    \n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    \n",
    "    # Process labels based on target\n",
    "    y_proc = []\n",
    "    \n",
    "    if target == 'pqc':\n",
    "        # Binary classification: 0 (Not Using PQC) vs 1 (Using PQC)\n",
    "        for label in y:\n",
    "            unit_digit = label % 10\n",
    "            if unit_digit == 0:\n",
    "                y_proc.append(0)  # Not using PQC\n",
    "            else:  # unit_digit is 1 or 2\n",
    "                y_proc.append(1)  # Using PQC\n",
    "        y_proc = np.array(y_proc)\n",
    "    elif target == 'browser':\n",
    "        # Extract browser info from tens digit: 10 (Firefox), 20 (Chrome)\n",
    "        y_proc = ((y // 10) % 10) * 10\n",
    "    elif target == 'os':\n",
    "        # Extract OS info from hundreds digit: 100 (Linux), 200 (Windows), 400 (MacOS)\n",
    "        y_proc = (y // 100) * 100\n",
    "    elif target == 'algo':\n",
    "        # Extract algorithm info from unit digit: 1 (Kyber), 2 (ML-KEM)\n",
    "        y_proc = y % 10\n",
    "    elif target == 'all':\n",
    "        # Keep original labels\n",
    "        y_proc = y\n",
    "    else:\n",
    "        print(f\"Unknown target: {target}\")\n",
    "        return None, None\n",
    "    \n",
    "    return X, y_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_organize(data, idx, labels, model, model_names, results):\n",
    "    \"\"\"\n",
    "    Performs stratified 10-fold cross-validation on the given model.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    accuracy = cross_val_score(model, data, labels, cv=skf, scoring='accuracy')\n",
    "    precision = cross_val_score(model, data, labels, cv=skf, scoring='precision_weighted')\n",
    "    recall = cross_val_score(model, data, labels, cv=skf, scoring='recall_weighted')\n",
    "    f1 = cross_val_score(model, data, labels, cv=skf, scoring='f1_weighted')\n",
    "    auc = cross_val_score(model, data, labels, cv=skf, scoring='roc_auc_ovr')\n",
    "\n",
    "    acc_res = f\"{accuracy.mean():.2f} +/- {accuracy.std():.2f}\"\n",
    "    prec_res = f\"{precision.mean():.2f} +/- {precision.std():.2f}\"\n",
    "    rec_res = f\"{recall.mean():.2f} +/- {recall.std():.2f}\"\n",
    "    f1_res = f\"{f1.mean():.2f} +/- {f1.std():.2f}\"\n",
    "    auc_res = f\"{auc.mean():.2f} +/- {auc.std():.2f}\"\n",
    "\n",
    "    results.loc[len(results)] = [\n",
    "        model_names[idx], acc_res, prec_res, rec_res, f1_res, auc_res\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(target: str, csv_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Runs multiple classification models on a given dataset.\n",
    "\n",
    "    This function loads and encodes the data for the specified target label and\n",
    "    number of packets. It then trains multiple classifiers, evaluates each model's\n",
    "    performance, and saves the results in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        target (str): The label or feature name to predict/classify.\n",
    "        amount (int): The number of packets or rows to include in the loaded dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load and encode data\n",
    "    data, labels = load_data(target, csv_path)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "    # Estimators and matching names\n",
    "    models = [\n",
    "        RandomForestClassifier(),\n",
    "        XGBClassifier(),\n",
    "        LogisticRegression(),\n",
    "        KNeighborsClassifier(),\n",
    "        DecisionTreeClassifier(),\n",
    "        MLPClassifier(),\n",
    "        GaussianNB(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier()\n",
    "    ]\n",
    "    model_names = [\n",
    "        'Random Forest',\n",
    "        'XGBoost',\n",
    "        'Logistic Regression',\n",
    "        'KNN',\n",
    "        'Decision Tree',\n",
    "        'MLP',\n",
    "        'Naive Bayes',\n",
    "        'AdaBoost',\n",
    "        'Gradient Boosting'\n",
    "    ]\n",
    "\n",
    "    # Sort models and names\n",
    "    model_names, models = zip(*sorted(zip(model_names, models)))\n",
    "\n",
    "    # Prepare results storage\n",
    "    results = pd.DataFrame(\n",
    "        columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC']\n",
    "    )\n",
    "\n",
    "    # Run each model\n",
    "    for idx, model in enumerate(models):\n",
    "        run_and_organize(data, idx, labels, model, model_names, results)\n",
    "\n",
    "    # Save to CSV\n",
    "    results.to_csv(f'docker-res-{target}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Set your parameters below and run the analysis. The available targets are:\n",
    "- 'pqc': Post-quantum cryptography classification\n",
    "- 'algo': Algorithm classification\n",
    "- 'tuple': Tuple classification\n",
    "- 'all': All features\n",
    "- 'browser': Browser classification\n",
    "- 'os': Operating system classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {110: 100, 111: 100, 112: 100, 120: 100, 121: 100, 122: 98}\n",
      "Total samples: 598\n",
      "Label distribution: {110: 100, 111: 100, 112: 100, 120: 100, 121: 100, 122: 98}\n",
      "Total samples: 398\n",
      "Label distribution: {110: 100, 111: 100, 112: 100, 120: 100, 121: 100, 122: 98}\n",
      "Total samples: 598\n",
      "Label distribution: {110: 100, 111: 100, 112: 100, 120: 100, 121: 100, 122: 98}\n",
      "Total samples: 398\n"
     ]
    }
   ],
   "source": [
    "# Set your parameters here\n",
    "targets = ['pqc', 'algo', 'all', 'browser']  # Choose from: 'pqc', 'algo', 'tuple', 'all', 'browser', 'os'\n",
    "csv_path = 'C:\\\\Users\\\\Eylon\\\\PQC\\\\tdl\\\\pqc-paob-docker-20.csv'  # Your CSV file path\n",
    "\n",
    "# Run the analysis\n",
    "for target in targets:\n",
    "    results = run_models(target, csv_path)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
