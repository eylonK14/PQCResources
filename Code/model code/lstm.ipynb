{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d191f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import ast\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb277617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target: str, csv_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load and preprocess data from a CSV file.\n",
    "\n",
    "    :param target: One of 'all', 'pqc', 'browser', 'os', 'algo', or 'tuple'.\n",
    "    :param csv_path: Path to the CSV file to load.\n",
    "    :return: Tuple (data, labels) where data is a numpy array and labels is another numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV at '{csv_path}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    labels = data.pop('label').values\n",
    "\n",
    "    # Log label counts\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(\"Label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "    rows = data.to_numpy()\n",
    "    filtered_rows, filtered_labels = [], []\n",
    "    for row, label in zip(rows, labels):\n",
    "        if target in ['browser', 'os'] and label % 2 == 0:\n",
    "            continue\n",
    "        filtered_rows.append([ast.literal_eval(cell) for cell in row])\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "    X = np.array([np.array(r).flatten() for r in filtered_rows])\n",
    "    y = np.array(filtered_labels)\n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "\n",
    "    # Process labels based on target\n",
    "    if target == 'algo':\n",
    "        y_proc = y // 10\n",
    "    elif target == 'tuple':\n",
    "        y_proc = y.copy()\n",
    "    else:\n",
    "        base = y\n",
    "        if target == 'pqc':\n",
    "            y_proc = base % 2\n",
    "        elif target == 'browser':\n",
    "            y_proc = ((base // 10) % 10) * 10\n",
    "        elif target == 'os':\n",
    "            y_proc = (base // 100) * 100\n",
    "        elif target == 'all':\n",
    "            y_proc = base\n",
    "        else:\n",
    "            print(f\"Unknown target: {target}\")\n",
    "            return None, None\n",
    "\n",
    "    return X, y_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b6c0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {210: 200, 211: 200, 220: 200, 221: 200, 310: 200, 311: 200, 320: 200, 321: 200, 410: 200, 411: 200, 420: 200, 421: 200}\n",
      "Total samples: 2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1,   60,    0, ...,    1,   52,  182],\n",
       "       [   1,  118,    0, ...,    0,  421,  825],\n",
       "       [   1,   60,    0, ...,    0,   83,  197],\n",
       "       ...,\n",
       "       [   1,  118,    0, ...,    0, 1420,  271],\n",
       "       [   1,   60,    0, ...,    0,  224,  225],\n",
       "       [   1,   64,    0, ...,    1,   52,  134]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = load_data(\"pqc\", \"C:\\\\Users\\\\Eylon\\\\PQC\\\\JournalDatasets\\\\pqc-pob.csv\")\n",
    "\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b60b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {210: 200, 211: 200, 220: 200, 221: 200, 310: 200, 311: 200, 320: 200, 321: 200, 410: 200, 411: 200, 420: 200, 421: 200}\n",
      "Total samples: 2400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_25 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.5110 - loss: 2.1575 - val_accuracy: 0.5375 - val_loss: 0.6782\n",
      "Epoch 2/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.5862 - loss: 0.6666 - val_accuracy: 0.5958 - val_loss: 0.6536\n",
      "Epoch 3/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.6621 - loss: 0.6224 - val_accuracy: 0.6208 - val_loss: 0.6266\n",
      "Epoch 4/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.7421 - loss: 0.5625 - val_accuracy: 0.6167 - val_loss: 0.6566\n",
      "Epoch 5/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7816 - loss: 0.4903 - val_accuracy: 0.6500 - val_loss: 0.6895\n",
      "Epoch 6/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8286 - loss: 0.3957 - val_accuracy: 0.7000 - val_loss: 0.8100\n",
      "Epoch 7/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.8679 - loss: 0.3151 - val_accuracy: 0.7500 - val_loss: 0.7826\n",
      "Epoch 8/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9151 - loss: 0.2049 - val_accuracy: 0.8792 - val_loss: 0.7778\n",
      "Epoch 9/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9558 - loss: 0.1603 - val_accuracy: 0.8958 - val_loss: 0.7645\n",
      "Epoch 10/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9819 - loss: 0.0988 - val_accuracy: 0.9042 - val_loss: 0.6247\n",
      "Epoch 11/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9938 - loss: 0.0699 - val_accuracy: 0.9208 - val_loss: 0.7641\n",
      "Epoch 12/12\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9967 - loss: 0.0579 - val_accuracy: 0.9000 - val_loss: 0.7628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17eeea0e120>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = load_data(\"pqc\", 'JournalDatasets\\\\pqc-pob.csv')\n",
    "\n",
    "seq_len = data.shape[1]           # אורך הרצף לל־LSTM\n",
    "max_id = int(np.max(data))        # האינדקס המקסימלי שמופיע בקלט\n",
    "vocab_size = max_id + 1   \n",
    "\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# create the model\n",
    "embedding_dim = 60  # זה ה- output_dim: כמה מאפיינים לכל טוקן\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,    # גודל אוצר מילים = max_id+1\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=seq_len))   # אורך הרצף\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=12, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7095f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "add4fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (raw): {0: 1200, 1: 1200}\n",
      "seq_len=3, feat_dim=20\n",
      "class_weight: {0: 1.0, 1: 1.0}\n",
      "Epoch 1/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.7595 - auc: 0.8335 - loss: 0.5679 - prauc: 0.8434 - val_accuracy: 0.8292 - val_auc: 0.9463 - val_loss: 0.4473 - val_prauc: 0.9558 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8847 - auc: 0.9691 - loss: 0.2262 - prauc: 0.9703 - val_accuracy: 0.8667 - val_auc: 0.9627 - val_loss: 0.3192 - val_prauc: 0.9662 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9020 - auc: 0.9769 - loss: 0.1830 - prauc: 0.9773 - val_accuracy: 0.8750 - val_auc: 0.9732 - val_loss: 0.2619 - val_prauc: 0.9756 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8896 - auc: 0.9755 - loss: 0.1805 - prauc: 0.9762 - val_accuracy: 0.8875 - val_auc: 0.9743 - val_loss: 0.2232 - val_prauc: 0.9767 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9102 - auc: 0.9814 - loss: 0.1652 - prauc: 0.9818 - val_accuracy: 0.8792 - val_auc: 0.9688 - val_loss: 0.2127 - val_prauc: 0.9711 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9129 - auc: 0.9831 - loss: 0.1578 - prauc: 0.9834 - val_accuracy: 0.8875 - val_auc: 0.9692 - val_loss: 0.1996 - val_prauc: 0.9711 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9152 - auc: 0.9843 - loss: 0.1518 - prauc: 0.9845 - val_accuracy: 0.8875 - val_auc: 0.9711 - val_loss: 0.1960 - val_prauc: 0.9733 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9169 - auc: 0.9830 - loss: 0.1554 - prauc: 0.9831 - val_accuracy: 0.8875 - val_auc: 0.9716 - val_loss: 0.1846 - val_prauc: 0.9736 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9217 - auc: 0.9863 - loss: 0.1402 - prauc: 0.9865 - val_accuracy: 0.8833 - val_auc: 0.9713 - val_loss: 0.1893 - val_prauc: 0.9730 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9230 - auc: 0.9870 - loss: 0.1373 - prauc: 0.9870 - val_accuracy: 0.8750 - val_auc: 0.9705 - val_loss: 0.1914 - val_prauc: 0.9724 - learning_rate: 5.0000e-04\n",
      "Accuracy: 88.75%\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, Model\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load CSV as (T, F) sequences instead of flattening\n",
    "# -----------------------------\n",
    "def load_data_lstm(target: str, csv_path: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_raw: list of 2D arrays, each (Ti, F) before padding\n",
    "      y_proc: np.ndarray of labels\n",
    "      feat_names: column names used as features (for debugging)\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_path)\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    # Extract labels and feature columns\n",
    "    labels = data.pop('label').values\n",
    "    feat_names = list(data.columns)\n",
    "\n",
    "    # Optional filter (kept as in your original function)\n",
    "    rows = data.to_numpy()\n",
    "    filtered_seqs, filtered_labels = [], []\n",
    "    for row, label in zip(rows, labels):\n",
    "        # If predicting browser/os you skipped even labels; keep your logic:\n",
    "        if target in ['browser', 'os'] and label % 2 == 0:\n",
    "            continue\n",
    "\n",
    "        # Parse each cell (string -> list/sequence)\n",
    "        # Each column is a feature with its own 1D time-series\n",
    "        feature_series = [np.array(ast.literal_eval(cell), dtype=np.float32) for cell in row]\n",
    "\n",
    "        # Align features along time: pad columns to the max length within this row\n",
    "        max_len = max(len(col) for col in feature_series)\n",
    "        F = len(feature_series)\n",
    "        seq = np.zeros((max_len, F), dtype=np.float32)\n",
    "        for j, col in enumerate(feature_series):\n",
    "            L = len(col)\n",
    "            seq[:L, j] = col  # right-padding with zeros\n",
    "        filtered_seqs.append(seq)\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "    y = np.array(filtered_labels)\n",
    "\n",
    "    # Process labels based on target (same semantics as your code)\n",
    "    if target == 'algo':\n",
    "        y_proc = y // 10\n",
    "    elif target == 'tuple':\n",
    "        y_proc = y.copy()\n",
    "    else:\n",
    "        base = y\n",
    "        if target == 'pqc':\n",
    "            y_proc = base % 2\n",
    "        elif target == 'browser':\n",
    "            y_proc = ((base // 10) % 10) * 10\n",
    "        elif target == 'os':\n",
    "            y_proc = (base // 100) * 100\n",
    "        elif target == 'all':\n",
    "            y_proc = base\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown target: {target}\")\n",
    "\n",
    "    print(\"Label distribution (raw):\", dict(zip(*np.unique(y_proc, return_counts=True))))\n",
    "    return filtered_seqs, y_proc, feat_names\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Pad to common length + scale features\n",
    "# -----------------------------\n",
    "def pad_and_scale(seqs_list, pad_to=None):\n",
    "    \"\"\"\n",
    "    seqs_list: list of (Ti, F) arrays\n",
    "    pad_to: optional fixed length; if None, use max Ti in list\n",
    "    Returns X: (N, T, F), scaler (fitted on train only later)\n",
    "    \"\"\"\n",
    "    N = len(seqs_list)\n",
    "    F = seqs_list[0].shape[1]\n",
    "    T = pad_to or max(s.shape[0] for s in seqs_list)\n",
    "    X = np.zeros((N, T, F), dtype=np.float32)\n",
    "    lengths = np.zeros(N, dtype=np.int32)\n",
    "    for i, s in enumerate(seqs_list):\n",
    "        L = min(s.shape[0], T)\n",
    "        X[i, :L, :] = s[:L, :]\n",
    "        lengths[i] = L\n",
    "    return X, lengths\n",
    "\n",
    "\n",
    "def scale_3d_fit_transform(X_train, mask_lengths):\n",
    "    \"\"\"\n",
    "    Fit StandardScaler column-wise (feature-wise) using unmasked timesteps only,\n",
    "    then transform both train and later test/val with the same scaler.\n",
    "    \"\"\"\n",
    "    N, T, F = X_train.shape\n",
    "    # Collect all valid rows across all sequences (exclude padded rows)\n",
    "    rows = []\n",
    "    for i in range(N):\n",
    "        L = mask_lengths[i]\n",
    "        if L > 0:\n",
    "            rows.append(X_train[i, :L, :])\n",
    "    all_valid = np.concatenate(rows, axis=0)  # (sum(L_i), F)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_valid)\n",
    "\n",
    "    # Apply scaler to all timesteps (padded zeros will become scaled zeros-ish)\n",
    "    Xs = X_train.reshape(-1, F)\n",
    "    Xs = scaler.transform(Xs)\n",
    "    Xs = Xs.reshape(N, T, F)\n",
    "    return Xs, scaler\n",
    "\n",
    "\n",
    "def scale_3d_transform(X, scaler):\n",
    "    N, T, F = X.shape\n",
    "    Xs = X.reshape(-1, F)\n",
    "    Xs = scaler.transform(Xs)\n",
    "    return Xs.reshape(N, T, F)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build a solid LSTM model\n",
    "# -----------------------------\n",
    "def build_lstm_model(seq_len, feat_dim):\n",
    "    inputs = layers.Input(shape=(seq_len, feat_dim))\n",
    "    x = layers.Masking(mask_value=0.0)(inputs)\n",
    "    x = layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.SpatialDropout1D(0.2)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(name=\"auc\", curve=\"ROC\"),\n",
    "            tf.keras.metrics.AUC(name=\"prauc\", curve=\"PR\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Train\n",
    "# -----------------------------\n",
    "# Load and prepare\n",
    "csv_path = r\"C:\\Users\\Eylon\\PQC\\JournalDatasets\\pqc-pob.csv\"  # <- adjust if needed\n",
    "target = \"pqc\"\n",
    "\n",
    "seqs_list, labels_raw, feat_names = load_data_lstm(target, csv_path)\n",
    "\n",
    "# Encode labels to {0,1} for binary\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels_raw)\n",
    "\n",
    "# Train/test split (TIP: consider GroupKFold by capture/session to avoid leakage)\n",
    "X_list_tr, X_list_te, y_tr, y_te = train_test_split(\n",
    "    seqs_list, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Pad (choose a max length; you can also cap with e.g., pad_to=512 for speed)\n",
    "X_tr, L_tr = pad_and_scale(X_list_tr, pad_to=None)\n",
    "X_te, L_te = pad_and_scale(X_list_te, pad_to=X_tr.shape[1])  # ensure same T\n",
    "\n",
    "# Scale features using only training timesteps\n",
    "X_tr, scaler = scale_3d_fit_transform(X_tr, L_tr)\n",
    "X_te = scale_3d_transform(X_te, scaler)\n",
    "\n",
    "seq_len, feat_dim = X_tr.shape[1], X_tr.shape[2]\n",
    "print(f\"seq_len={seq_len}, feat_dim={feat_dim}\")\n",
    "\n",
    "# Class weights (if imbalanced)\n",
    "classes = np.unique(y_tr)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_tr)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# Build & train\n",
    "model = build_lstm_model(seq_len, feat_dim)\n",
    "\n",
    "cbs = [\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", patience=3, factor=0.5, min_lr=1e-5),\n",
    "    callbacks.ModelCheckpoint(\"best_lstm.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_te, y_te),\n",
    "    epochs=60,\n",
    "    batch_size=64,\n",
    "    callbacks=cbs,\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_te, y_te, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
